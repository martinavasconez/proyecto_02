{"block_file": {"custom/wise_arcanist.py:custom:python:wise arcanist": {"content": "import snowflake.connector\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives import serialization\n\n# Lee la clave privada directamente desde un string multil\u00ednea\nprivate_key_str = \"\"\"-----BEGIN PRIVATE KEY-----\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCzHaaaSYqeAPfT\ntmw6zesHokPoqzL7o6UN4s3Dhx8V2g0VVCw4eWxVr0K0cxvn0xu0OKsUI1vrA7G0\n6XWagyJkGmpz+HzE3W4lDuakB+OoOh7CDNcHH9FbWyAsqKmunHgwClYheOf0otv4\nY46fU1wg6vN2SdCFzGNgDLOSh+Rvl+z1RPtSQLt5q4Rx0TTm8q9XN7tYWXNAKjwK\nZKRqUR17tZtaUDE/truQGeoOLnXWgqg0OLxzvxtmYJqG7x+0rEKEzCwz9CwZnROc\ni8rg8ML6df0tAi2vTgfSJmPN1fbFPNKsie7pjAxJyF2Hboa84j3vgEmdwjfN14FA\nH7hJnu4/AgMBAAECggEACYBMv1AGjsCvMYdltyDUYAB9gn2ovXk9+Ir++zY5sedx\n9jjgatNJQ3cbUYZU0dFJXbmVaZx09ClEze65LEva3fbnJ37kFdVHhUCEsrX9OR+l\nutY17QAFE2GCejWTYQ3JWlMFPc7m28hsmE2mDR5XSFYhbM7kxS3FJh0sxI/gA2mz\nlYPxYGYOpSTazHqpgDw0OSiwGWGyhHxbVj0dOJiATleq0anji678nLBHx/TIlfVe\n7xcmkKlGBzF3ZR7UmKCrQs7SsGkiJNNmP6Qxapd5wy42QGAhIK2HYUkqSHILgpWs\nMQMyBNSDdiH/2yn1MgTTPxCKA1xt6XoRiQb6CdJkgQKBgQDze3v4kRoca5X/UtMM\nzVP1ZcyhE7R353nc38tiJxvHfc4I+jSzqr+fdRv2zueNvMbr3C9yjFMiYAryfOXI\nW8rWIWZKN2+poBw1/zf+d0Dcxg/TmBYLuYW73td9lMb3ua8KZEweeOx2/XrP6bo/\nxd0uzf5l+DlbD8NsCa7K4O4SIQKBgQC8Uwbcb9X0F/0TcQIETd3nlmRS0W2zZukd\n2Tz0bq0Q6jNdQc3fuwMqNY3lNXTCQQ9ak2InElnG+Xhe8hoQQF5afP0lSlLG50J8\n1axS0hJQdlKs0qI0sYO4K3g6YJBmEIDw/56HpyLgIIMxZItWMf0Z63jL4hewGC9f\na11oeae0XwKBgGEQIxWuUqhtwzgrvnLmD8hOMssr3c/G+W+xz5RrXsmiP1aY2BWf\nxhA9UU6MoQaB8RLpjgiuJB4aB4MvgzLiVPQUEIEZpGwMpfJosdvBkpvwYTLK+E7o\nQIXqiiFIBCGRZlRQM4AaWLn+xszHsjXmHQyhlf70e3jvycnx+jpqfL3BAoGAd7lQ\n41M18bhOa82sOpBGQrSZkw0RcLw7933kAoFaBSbfAKqU92cs2+iwDMevMs+psyt/\netdvu89ddv7zEuHZGi3bwZk+hrT+z94Hb5+dhQm0Bari9BzmYG9CP9qj8j8LuirH\n3fWjdlk1DnGdI28kORY59WQHKyw08bSP7ZtpwoECgYEAn/3ZFXxwKQYDPAcY5wFB\n6l5Jvi0NOr58gdRyF5F7u5YvCyfh5eMDeYVTJ8gF/MDe5ltH0YsIRQjNlPQM4iIn\nf4poLaHwP9hE9lhFP/nNSgDJnxlga37sDdS+MvZISGCkfq2oi23ES17s3Ii2/kLy\nuGYseec2zhVuBnLKenJtxIQ=\n-----END PRIVATE KEY-----\n\"\"\"\n\n# Convertimos a objeto usable\np_key = serialization.load_pem_private_key(\n    private_key_str.encode(\"utf-8\"),\n    password=None,  # o pon aqu\u00ed tu passphrase si la clave tiene\n    backend=default_backend()\n)\n\n# Serializamos a formato DER\nprivate_key_bytes = p_key.private_bytes(\n    encoding=serialization.Encoding.DER,\n    format=serialization.PrivateFormat.PKCS8,\n    encryption_algorithm=serialization.NoEncryption()\n)\n\n# Conexi\u00f3n con Snowflake\nconn = snowflake.connector.connect(\n    user=\"MARTINASERVICEUSER\",\n    account=\"HFISOEI-XY68658\",\n    private_key=private_key_bytes,\n    warehouse=\"COMPUTE_WH\",\n    database=\"NY_TAXI\",\n    schema=\"RAW\",\n    role=\"ROLE_INGESTA\"\n)\n\ncur = conn.cursor()\ncur.execute(\"SELECT CURRENT_USER(), CURRENT_ROLE(), CURRENT_WAREHOUSE(), CURRENT_DATABASE(), CURRENT_SCHEMA()\")\nprint(cur.fetchone())\n", "file_path": "custom/wise_arcanist.py", "language": "python", "type": "custom", "uuid": "wise_arcanist"}, "data_exporters/brilliant_shadow.py:data_exporter:python:brilliant shadow": {"content": "if 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data(data, *args, **kwargs):\n    \"\"\"\n    Exports data to some source.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Output (optional):\n        Optionally return any object and it'll be logged and\n        displayed when inspecting the block run.\n    \"\"\"\n    # Specify your data exporting logic here\n\n\n", "file_path": "data_exporters/brilliant_shadow.py", "language": "python", "type": "data_exporter", "uuid": "brilliant_shadow"}, "data_exporters/export_data.py:data_exporter:python:export data": {"content": "if 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\nimport snowflake.connector\nfrom snowflake.connector.pandas_tools import write_pandas\nfrom mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom os import path\n\n\ndef get_snowflake_conn():\n    config_path = path.join(get_repo_path(), \"io_config.yaml\")\n    config_profile = \"default\"\n    config_loader = ConfigFileLoader(config_path, config_profile)\n\n    print(\"[INFO] Conectando a Snowflake...\")\n    conn = snowflake.connector.connect(\n        user=config_loader.get(\"SNOWFLAKE_USER\"),\n        password=config_loader.get(\"SNOWFLAKE_PASSWORD\"),\n        account=config_loader.get(\"SNOWFLAKE_ACCOUNT\"),\n        warehouse=config_loader.get(\"SNOWFLAKE_DEFAULT_WH\"),\n        database=config_loader.get(\"SNOWFLAKE_DEFAULT_DB\"),\n        schema=config_loader.get(\"SNOWFLAKE_DEFAULT_SCHEMA\"),\n        role=config_loader.get(\"SNOWFLAKE_ROLE\"),\n    )\n    print(\"[INFO] Conexi\u00f3n establecida\")\n    return conn\n\n\n@data_exporter\ndef export_data(data, *args, **kwargs):\n\n    table_name = \"TAXI_ZONES\"  \n    conn = get_snowflake_conn()\n\n    print(f\"[INFO] Exportando DataFrame a la tabla {table_name}\")\n    print(f\"[INFO] DataFrame con {len(data)} filas y {len(data.columns)} columnas\")\n\n\n    success, nchunks, nrows, _ = write_pandas(\n        conn,\n        data,\n        table_name=table_name,\n        auto_create_table=True,\n        overwrite=False,\n        quote_identifiers=True\n    )\n\n    if success:\n        print(f\"[OK] {nrows} filas exportadas a Snowflake ({nchunks} chunk(s)).\")\n    else:\n        print(\"[ERROR] Exportaci\u00f3n fallida\")\n\n    conn.close()\n    print(\"[INFO] Conexi\u00f3n cerrada\")\n", "file_path": "data_exporters/export_data.py", "language": "python", "type": "data_exporter", "uuid": "export_data"}, "data_exporters/weathered_song.py:data_exporter:python:weathered song": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.snowflake import Snowflake\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_snowflake(batch: dict, **kwargs) -> None:\n    config_path = path.join(get_repo_path(), \"io_config.yaml\")\n    config_profile = \"default\"\n    database = \"NY_TAXI\"\n    schema = \"RAW\"\n    table_name = \"testing\"\n\n    trips_df: DataFrame = batch.get(\"trips\")\n\n    if trips_df is not None and not trips_df.empty:\n        print(f\"[EXPORT] Subiendo {len(trips_df)} filas a {schema}.{table_name}\")\n\n        with Snowflake.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n            loader.export(\n                trips_df.rename(columns=str.upper),\n                table_name,\n                database,\n                schema,\n                if_exists=\"append\",\n                chunk_size=100_000,\n            )\n\n        print(f\"[OK] {len(trips_df)} filas insertadas en {schema}.{table_name}\")\n", "file_path": "data_exporters/weathered_song.py", "language": "python", "type": "data_exporter", "uuid": "weathered_song"}, "data_exporters/export_titanic_clean.py:data_exporter:python:export titanic clean": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to filesystem.\n\n    Docs: https://docs.mage.ai/design/data-loading#example-loading-data-from-a-file\n    \"\"\"\n    filepath = 'titanic_clean.csv'\n    FileIO().export(df, filepath)\n", "file_path": "data_exporters/export_titanic_clean.py", "language": "python", "type": "data_exporter", "uuid": "export_titanic_clean"}, "data_exporters/exporter_trips.py:data_exporter:python:exporter trips": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.snowflake import Snowflake\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_snowflake(dfs: dict, **kwargs) -> None:\n    config_path = path.join(get_repo_path(), \"io_config.yaml\")\n    config_profile = \"default\"\n\n    database = \"NY_TAXI\"\n    schema = \"RAW\"\n    table_name = \"TRIPS_yelllow\"\n\n    trips_df: DataFrame = dfs.get(\"trips\")\n    if trips_df is None or trips_df.empty:\n        print(\"[WARN] No hay datos en trips para exportar.\")\n        return\n\n    trips_df = trips_df.rename(columns=str.upper)\n\n    with Snowflake.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        cols = \", \".join([f\"{col} STRING\" for col in trips_df.columns])\n        create_sql = f\"CREATE TABLE IF NOT EXISTS {schema}.{table_name} ({cols});\"\n        loader.execute(create_sql)\n\n        loader.export(\n            trips_df,\n            table_name,\n            database,\n            schema,\n            if_exists=\"append\",  \n            chunk_size=100000,\n        )\n\n        print(f\"[OK] Export completado \u2192 {schema}.{table_name}, filas insertadas: {len(trips_df)}\")\n", "file_path": "data_exporters/exporter_trips.py", "language": "python", "type": "data_exporter", "uuid": "exporter_trips"}, "data_exporters/carefree_arcanist.py:data_exporter:python:carefree arcanist": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_postgres(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a PostgreSQL database.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#postgresql\n    \"\"\"\n    schema_name = 'your_schema_name'  # Specify the name of the schema to export data to\n    table_name = 'your_table_name'  # Specify the name of the table to export data to\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        loader.export(\n            df,\n            schema_name,\n            table_name,\n            index=False,  # Specifies whether to include index in exported table\n            if_exists='replace',  # Specify resolution policy if table name already exists\n        )\n", "file_path": "data_exporters/carefree_arcanist.py", "language": "python", "type": "data_exporter", "uuid": "carefree_arcanist"}, "data_loaders/load_titanic.py:data_loader:python:load titanic": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv?raw=True'\n\n    return pd.read_csv(url)\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_titanic.py", "language": "python", "type": "data_loader", "uuid": "load_titanic"}, "data_loaders/ingest_taxi_zones.py:data_loader:python:ingest taxi zones": {"content": "if 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nimport pandas as pd\n\n@data_loader\ndef load_data(*args, **kwargs):\n\n    url = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\"\n    print(f\"[INFO] Descargando datos desde {url}\")\n\n    data_raw = pd.read_csv(url)\n    print(f\"[INFO] Archivo cargado correctamente con {len(data_raw)} filas y {len(data_raw.columns)} columnas\")\n\n    return data_raw\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/ingest_taxi_zones.py", "language": "python", "type": "data_loader", "uuid": "ingest_taxi_zones"}, "data_loaders/ingest_taxi.py:data_loader:python:ingest taxi": {"content": "if 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nimport pandas as pd\nimport pyarrow.parquet as pq\nimport requests\nfrom io import BytesIO\nfrom datetime import datetime\nimport uuid\nimport time\nimport gc\nfrom os import path\nimport snowflake.connector\nfrom snowflake.connector.pandas_tools import write_pandas\nfrom mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\n\n\nBASE_URL = \"https://d37ci6vzurychx.cloudfront.net/trip-data\"\nHEADERS = {\n    \"User-Agent\": \"Mozilla/5.0\",\n    \"Accept\": \"application/octet-stream\",\n}\n\n\ndef _build_url(service: str, year: int, month: int) -> str:\n    return f\"{BASE_URL}/{service}_tripdata_{year}-{month:02d}.parquet\"\n\n\ndef get_snowflake_conn():\n    config_path = path.join(get_repo_path(), \"io_config.yaml\")\n    config_profile = \"default\"\n    config_loader = ConfigFileLoader(config_path, config_profile)\n\n    return snowflake.connector.connect(\n        user=config_loader.get(\"SNOWFLAKE_USER\"),\n        password=config_loader.get(\"SNOWFLAKE_PASSWORD\"),\n        account=config_loader.get(\"SNOWFLAKE_ACCOUNT\"),\n        warehouse=config_loader.get(\"SNOWFLAKE_DEFAULT_WH\"),\n        database=config_loader.get(\"SNOWFLAKE_DEFAULT_DB\"),\n        schema=config_loader.get(\"SNOWFLAKE_DEFAULT_SCHEMA\"),\n        role=config_loader.get(\"SNOWFLAKE_ROLE\"),\n    )\n\n\ndef create_table_if_not_exists(conn, table_name: str, audit_table: str):\n    create_sql = f\"\"\"\n    CREATE TABLE IF NOT EXISTS {table_name} (\n    VENDORID NUMBER,\n    LPEP_PICKUP_DATETIME TIMESTAMP_NTZ,\n    LPEP_DROPOFF_DATETIME TIMESTAMP_NTZ,\n    STORE_AND_FWD_FLAG VARCHAR(1),\n    RATECODEID NUMBER,\n    PULOCATIONID NUMBER,\n    DOLOCATIONID NUMBER,\n    PASSENGER_COUNT NUMBER,\n    TRIP_DISTANCE FLOAT,\n    FARE_AMOUNT FLOAT,\n    EXTRA FLOAT,\n    MTA_TAX FLOAT,\n    TIP_AMOUNT FLOAT,\n    TOLLS_AMOUNT FLOAT,\n    EHAIL_FEE FLOAT,\n    IMPROVEMENT_SURCHARGE FLOAT,\n    TOTAL_AMOUNT FLOAT,\n    PAYMENT_TYPE NUMBER,\n    TRIP_TYPE NUMBER,\n    CONGESTION_SURCHARGE FLOAT,\n    CBD_CONGESTION_FEE FLOAT,\n    AIRPORT_FEE FLOAT,\n    RUN_ID STRING,\n    VENTANA_TEMPORAL TIMESTAMP_NTZ,\n    CHUNK_MES VARCHAR(50),\n    YEAR NUMBER,\n    MONTH NUMBER,\n    PRIMARY KEY (LPEP_PICKUP_DATETIME, LPEP_DROPOFF_DATETIME, PULOCATIONID, DOLOCATIONID)\n);\n     \"\"\"\n    cur = conn.cursor()\n    cur.execute(create_sql)\n\n    # tabla de auditor\u00eda\n    create_audit_sql = f\"\"\"\n    CREATE TABLE IF NOT EXISTS {audit_table} (\n        SERVICE STRING,\n        YEAR NUMBER,\n        MONTH NUMBER,\n        STATUS STRING,\n        ROW_COUNT NUMBER,\n        ERROR_MESSAGE STRING,\n        RUN_ID STRING,\n        INGESTED_AT_UTC TIMESTAMP_NTZ\n    );\n    \"\"\"\n    cur.execute(create_audit_sql)\n    cur.close()\n\n\ndef month_already_loaded(conn, audit_table: str, service: str, year: int, month: int):\n    query = f\"\"\"\n    SELECT 1\n    FROM {audit_table}\n    WHERE SERVICE = %s\n      AND YEAR = %s\n      AND MONTH = %s\n      AND STATUS = 'ok'\n    LIMIT 1\n    \"\"\"\n    cur = conn.cursor()\n    cur.execute(query, (service, year, month))\n    result = cur.fetchone()\n    cur.close()\n    return result is not None\n\n\n@data_loader\ndef load_data(*args, **kwargs):\n    service = \"green\"\n    start_year = int(kwargs.get(\"start_year\"))\n    end_year = int(kwargs.get(\"end_year\"))\n\n    run_id = str(uuid.uuid4())\n    ingested_at = kwargs.get(\"execution_date\") or datetime.utcnow()\n\n    conn = get_snowflake_conn()\n    table_name = \"GREEN_TAXIS\"\n    audit_table = \"AUDIT_GREEN\"\n\n    create_table_if_not_exists(conn, table_name, audit_table)\n\n    audit_rows = []\n\n    try:\n        for year in range(start_year, end_year + 1):\n            for month in range(1, 13):  \n                if month_already_loaded(conn, audit_table, service, year, month):\n                    print(f\"[SKIP] {service} {year}-{month:02d} ya cargado, se omite.\")\n                    continue\n\n                url = _build_url(service, year, month)\n                print(f\"\\n[INFO] Ingestando {service} {year}-{month:02d} desde {url}\")\n\n                row_count = 0\n                try:\n                    resp = requests.get(url, headers=HEADERS, timeout=60)\n                    if resp.status_code == 403:\n                        print(f\"[SKIP] {year}-{month:02d} no disponible (404)\")\n                        audit_rows.append({\n                        \"service\": service,\n                        \"year\": year,\n                        \"month\": month,\n                        \"status\": \"skip\",\n                        \"row_count\": 0,\n                        \"error_message\": \"Archivo no disponible (404)\",\n                        \"run_id\": run_id,\n                        \"ingested_at_utc\": ingested_at,\n                    })\n                        continue\n                    \n                    resp.raise_for_status()\n\n                    parquet_file = pq.ParquetFile(BytesIO(resp.content))\n                    total_rows = parquet_file.metadata.num_rows\n                    total_batches = (total_rows // 1_000_000) + (1 if total_rows % 1_000_000 else 0)\n\n                    print(f\"[INFO] {year}-{month:02d} tiene {total_rows} filas en {total_batches} chunks\")\n\n                    for i, batch in enumerate(parquet_file.iter_batches(batch_size=1_000_000), start=1):\n                        df = batch.to_pandas()\n\n                        # metadata\n                        df[\"RUN_ID\"] = run_id\n                        df[\"VENTANA_TEMPORAL\"] = datetime.now()\n                        df[\"CHUNK_MES\"] = f\"{i}/{month}\"\n                        df[\"YEAR\"] = year\n                        df[\"MONTH\"] = month\n                        df = df.rename(columns=str.upper)\n\n                        time.sleep(1)\n\n\n                        for col in [\"LPEP_PICKUP_DATETIME\", \"LPEP_DROPOFF_DATETIME\", \"VENTANA_TEMPORAL\"]:\n                            if col in df.columns:\n                                df[col] = pd.to_datetime(df[col], errors=\"coerce\").dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n\n                        # exportar\n                        success, nchunks, nrows, _ = write_pandas(\n                            conn,\n                            df,\n                            table_name=table_name,\n                            auto_create_table=False,\n                            overwrite=False,\n                            quote_identifiers=True,\n                        )\n\n                        row_count += len(df)\n                        restantes = total_batches - i\n                        print(\n                            f\"[EXPORT] {year}-{month:02d} \u2192 chunk {i}/{total_batches} \"\n                            f\"({len(df)} filas, acumulado {row_count}, faltan {restantes})\"\n                        )\n\n                        del df\n                        gc.collect()\n                        time.sleep(1)\n\n                    # insertar en audit\n                    cur = conn.cursor()\n                    cur.execute(f\"\"\"\n                        INSERT INTO {audit_table}\n                        (SERVICE, YEAR, MONTH, STATUS, ROW_COUNT, ERROR_MESSAGE, RUN_ID, INGESTED_AT_UTC)\n                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n                    \"\"\", (service, year, month, 'ok', row_count, None, run_id, ingested_at))\n                    cur.close()\n\n                    print(f\"[OK] {year}-{month:02d} exportado \u2192 {row_count} filas totales\")\n\n                except Exception as e:\n                    cur = conn.cursor()\n                    cur.execute(f\"\"\"\n                        INSERT INTO {audit_table}\n                        (SERVICE, YEAR, MONTH, STATUS, ROW_COUNT, ERROR_MESSAGE, RUN_ID, INGESTED_AT_UTC)\n                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n                    \"\"\", (service, year, month, 'fail', 0, str(e), run_id, ingested_at))\n                    cur.close()\n                    print(f\"[ERROR] {year}-{month:02d} \u2192 {e}\")\n\n                time.sleep(1)\n\n    finally:\n        conn.close()\n\n    return {\"audit\": pd.DataFrame(audit_rows)}", "file_path": "data_loaders/ingest_taxi.py", "language": "python", "type": "data_loader", "uuid": "ingest_taxi"}, "data_loaders/test.py:data_loader:python:test": {"content": "if 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\nimport pandas as pd\n\n\n@data_loader\ndef load_data(*args, **kwargs):\n    url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-06.parquet\"\n    data = pd.read_parquet(url)\n    return data\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/test.py", "language": "python", "type": "data_loader", "uuid": "test"}, "data_loaders/green_portal.py:data_loader:python:green portal": {"content": "if 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\nimport pandas as pd\n\n\n@data_loader\ndef load_data(*args, **kwargs):\n    url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-01.parquet\"\n    data = pd.read_parquet(url)\n    return data\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'", "file_path": "data_loaders/green_portal.py", "language": "python", "type": "data_loader", "uuid": "green_portal"}, "data_loaders/ingest_taxi_yellow.py:data_loader:python:ingest taxi yellow": {"content": "\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nimport pandas as pd\nimport pyarrow.parquet as pq\nimport requests\nfrom io import BytesIO\nfrom datetime import datetime\nimport uuid\nimport time\nimport gc\nfrom os import path\nimport snowflake.connector\nfrom snowflake.connector.pandas_tools import write_pandas\nfrom mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\n\n\nBASE_URL = \"https://d37ci6vzurychx.cloudfront.net/trip-data\"\nHEADERS = {\n    \"User-Agent\": \"Mozilla/5.0\",\n    \"Accept\": \"application/octet-stream\",\n}\n\n\ndef _build_url(service: str, year: int, month: int) -> str:\n    return f\"{BASE_URL}/{service}_tripdata_{year}-{month:02d}.parquet\"\n\ndef get_snowflake_conn():\n    config_path = path.join(get_repo_path(), \"io_config.yaml\")\n    config_profile = \"default\"\n    config_loader = ConfigFileLoader(config_path, config_profile)\n\n    return snowflake.connector.connect(\n        user=config_loader.get(\"SNOWFLAKE_USER\"),\n        password=config_loader.get(\"SNOWFLAKE_PASSWORD\"),\n        account=config_loader.get(\"SNOWFLAKE_ACCOUNT\"),\n        warehouse=config_loader.get(\"SNOWFLAKE_DEFAULT_WH\"),\n        database=config_loader.get(\"SNOWFLAKE_DEFAULT_DB\"),\n        schema=config_loader.get(\"SNOWFLAKE_DEFAULT_SCHEMA\"),\n        role=config_loader.get(\"SNOWFLAKE_ROLE\"),\n    )\n\ndef create_table_if_not_exists(conn, table_name: str):\n    create_sql = f\"\"\"\n    CREATE TABLE IF NOT EXISTS {table_name} (\n        VENDORID NUMBER,\n        TPEP_PICKUP_DATETIME TIMESTAMP_NTZ,\n        TPEP_DROPOFF_DATETIME TIMESTAMP_NTZ,\n        PASSENGER_COUNT NUMBER,\n        TRIP_DISTANCE FLOAT,\n        RATECODEID NUMBER,\n        STORE_AND_FWD_FLAG STRING,\n        PULOCATIONID NUMBER,\n        DOLOCATIONID NUMBER,\n        PAYMENT_TYPE NUMBER,\n        FARE_AMOUNT FLOAT,\n        EXTRA FLOAT,\n        MTA_TAX FLOAT,\n        TIP_AMOUNT FLOAT,\n        TOLLS_AMOUNT FLOAT,\n        IMPROVEMENT_SURCHARGE FLOAT,\n        TOTAL_AMOUNT FLOAT,\n        CONGESTION_SURCHARGE FLOAT,\n        AIRPORT_FEE FLOAT,\n        RUN_ID STRING,\n        WINDOW_START TIMESTAMP_NTZ,\n        WINDOW_END TIMESTAMP_NTZ,\n        SERVICE_TYPE STRING,\n        YEAR NUMBER,\n        MONTH NUMBER,\n        SOURCE_URL STRING,\n        INGESTED_AT_UTC TIMESTAMP_NTZ\n    )\n    \"\"\"\n    cur = conn.cursor()\n    cur.execute(create_sql)\n    cur.close()\n\n@data_loader\ndef load_data(*args, **kwargs):\n    service = \"yellow\"\n    start_year = int(kwargs.get(\"start_year\"))\n    end_year = int(kwargs.get(\"end_year\"))\n\n    run_id = str(uuid.uuid4())\n    ingested_at = kwargs.get(\"execution_date\") or datetime.utcnow()\n\n    conn = get_snowflake_conn()\n    table_name = \"TRIPS\"\n\n    create_table_if_not_exists(conn, table_name)\n\n    audit_rows = []\n\n    try:\n        for year in range(start_year, end_year + 1):\n            for month in range(1, 2):  # prueba solo enero\n                url = _build_url(service, year, month)\n                print(f\"\\n[INFO] Ingestando {service} {year}-{month:02d} desde {url}\")\n\n                row_count = 0\n                try:\n                    resp = requests.get(url, headers=HEADERS, timeout=60)\n                    resp.raise_for_status()\n\n                    parquet_file = pq.ParquetFile(BytesIO(resp.content))\n                    total_rows = parquet_file.metadata.num_rows\n                    total_batches = (total_rows // 1_000_000) + (1 if total_rows % 1_000_000 else 0)\n\n                    print(f\"[INFO] {year}-{month:02d} tiene {total_rows} filas en {total_batches} chunks\")\n\n                    for i, batch in enumerate(parquet_file.iter_batches(batch_size=1_000_000), start=1):\n                        df = batch.to_pandas()\n\n                        # metadata\n                        df[\"RUN_ID\"] = run_id\n                        df[\"WINDOW_START\"] = datetime(year, month, 1)\n                        df[\"WINDOW_END\"] = (\n                            datetime(year + 1, 1, 1) - pd.Timedelta(seconds=1)\n                            if month == 12\n                            else datetime(year, month + 1, 1) - pd.Timedelta(seconds=1)\n                        )\n                        df[\"SERVICE_TYPE\"] = service\n                        df[\"YEAR\"] = year\n                        df[\"MONTH\"] = month\n                        df[\"SOURCE_URL\"] = url\n                        df[\"INGESTED_AT_UTC\"] = ingested_at\n                        df = df.rename(columns=str.upper)\n\n\n                        # exportar\n                        success, nchunks, nrows, _ = write_pandas(\n                            conn,\n                            df,\n                            table_name=table_name,\n                            auto_create_table=False,\n                            overwrite=False,\n                        )\n\n                        row_count += len(df)\n                        restantes = total_batches - i\n                        print(\n                            f\"[EXPORT] {year}-{month:02d} \u2192 chunk {i}/{total_batches} \"\n                            f\"({len(df)} filas, acumulado {row_count}, faltan {restantes})\"\n                        )\n\n                        del df\n                        gc.collect()\n                        time.sleep(0.5)\n\n                    audit_rows.append({\n                        \"service\": service,\n                        \"year\": year,\n                        \"month\": month,\n                        \"status\": \"ok\",\n                        \"row_count\": row_count,\n                        \"error_message\": None,\n                        \"run_id\": run_id,\n                        \"ingested_at_utc\": ingested_at,\n                    })\n                    print(f\"[OK] {year}-{month:02d} exportado \u2192 {row_count} filas totales\")\n\n                except Exception as e:\n                    audit_rows.append({\n                        \"service\": service,\n                        \"year\": year,\n                        \"month\": month,\n                        \"status\": \"fail\",\n                        \"row_count\": 0,\n                        \"error_message\": str(e),\n                        \"run_id\": run_id,\n                        \"ingested_at_utc\": ingested_at,\n                    })\n                    print(f\"[ERROR] {year}-{month:02d} \u2192 {e}\")\n                time.sleep(1)\n\n    finally:\n        conn.close()\n\n    return {\"audit\": pd.DataFrame(audit_rows)}\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/ingest_taxi_yellow.py", "language": "python", "type": "data_loader", "uuid": "ingest_taxi_yellow"}, "data_loaders/ingest_yellow_taxis.py:data_loader:python:ingest yellow taxis": {"content": "from mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nimport pandas as pd\nimport pyarrow.parquet as pq\nimport requests\nfrom io import BytesIO\nfrom datetime import datetime\nimport uuid\nimport time\nimport gc\nfrom os import path\nimport snowflake.connector\nfrom snowflake.connector.pandas_tools import write_pandas\nfrom mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\n\n\nBASE_URL = \"https://d37ci6vzurychx.cloudfront.net/trip-data\"\nHEADERS = {\n    \"User-Agent\": \"Mozilla/5.0\",\n    \"Accept\": \"application/octet-stream\",\n}\n\n\ndef _build_url(service: str, year: int, month: int) -> str:\n    return f\"{BASE_URL}/{service}_tripdata_{year}-{month:02d}.parquet\"\n\n\ndef get_snowflake_conn():\n    config_path = path.join(get_repo_path(), \"io_config.yaml\")\n    config_profile = \"default\"\n    config_loader = ConfigFileLoader(config_path, config_profile)\n\n    return snowflake.connector.connect(\n        user=config_loader.get(\"SNOWFLAKE_USER\"),\n        password=config_loader.get(\"SNOWFLAKE_PASSWORD\"),\n        account=config_loader.get(\"SNOWFLAKE_ACCOUNT\"),\n        warehouse=config_loader.get(\"SNOWFLAKE_DEFAULT_WH\"),\n        database=config_loader.get(\"SNOWFLAKE_DEFAULT_DB\"),\n        schema=config_loader.get(\"SNOWFLAKE_DEFAULT_SCHEMA\"),\n        role=config_loader.get(\"SNOWFLAKE_ROLE\"),\n    )\n\n\ndef create_table_if_not_exists(conn, table_name: str, audit_table: str):\n    create_sql = f\"\"\"\n    CREATE TABLE IF NOT EXISTS {table_name} (\n    VENDORID NUMBER,\n    TPEP_PICKUP_DATETIME TIMESTAMP_NTZ,\n    TPEP_DROPOFF_DATETIME TIMESTAMP_NTZ,\n    PASSENGER_COUNT NUMBER,\n    TRIP_DISTANCE FLOAT,\n    RATECODEID NUMBER,\n    STORE_AND_FWD_FLAG VARCHAR(1),\n    PULOCATIONID NUMBER,\n    DOLOCATIONID NUMBER,\n    PAYMENT_TYPE NUMBER,\n    FARE_AMOUNT FLOAT,\n    EXTRA FLOAT,\n    MTA_TAX FLOAT,\n    TIP_AMOUNT FLOAT,\n    TOLLS_AMOUNT FLOAT,\n    IMPROVEMENT_SURCHARGE FLOAT,\n    TOTAL_AMOUNT FLOAT,\n    CONGESTION_SURCHARGE FLOAT,\n    AIRPORT_FEE FLOAT,\n    CBD_CONGESTION_FEE FLOAT,\n    RUN_ID STRING,\n    VENTANA_TEMPORAL TIMESTAMP_NTZ,\n    CHUNK_MES VARCHAR(50),\n    YEAR NUMBER,\n    MONTH NUMBER,\n    PRIMARY KEY (TPEP_PICKUP_DATETIME, TPEP_DROPOFF_DATETIME, PULOCATIONID, DOLOCATIONID)\n);\n     \"\"\"\n    cur = conn.cursor()\n    cur.execute(create_sql)\n\n    # tabla de auditor\u00eda\n    create_audit_sql = f\"\"\"\n    CREATE TABLE IF NOT EXISTS {audit_table} (\n        SERVICE STRING,\n        YEAR NUMBER,\n        MONTH NUMBER,\n        STATUS STRING,\n        ROW_COUNT NUMBER,\n        ERROR_MESSAGE STRING,\n        RUN_ID STRING,\n        INGESTED_AT_UTC TIMESTAMP_NTZ\n    );\n    \"\"\"\n    cur.execute(create_audit_sql)\n    cur.close()\n\n\ndef month_already_loaded(conn, audit_table: str, service: str, year: int, month: int):\n    query = f\"\"\"\n    SELECT 1\n    FROM {audit_table}\n    WHERE SERVICE = %s\n      AND YEAR = %s\n      AND MONTH = %s\n      AND STATUS = 'ok'\n    LIMIT 1\n    \"\"\"\n    cur = conn.cursor()\n    cur.execute(query, (service, year, month))\n    result = cur.fetchone()\n    cur.close()\n    return result is not None\n\n\n@data_loader\ndef load_data(*args, **kwargs):\n    service = \"yellow\"\n    start_year = int(kwargs.get(\"start_year\"))\n    end_year = int(kwargs.get(\"end_year\"))\n\n    run_id = str(uuid.uuid4())\n    ingested_at = kwargs.get(\"execution_date\") or datetime.utcnow()\n\n    conn = get_snowflake_conn()\n    table_name = \"YELLOW_TAXIS\"\n    audit_table = \"AUDIT_YELLOW\"\n\n    create_table_if_not_exists(conn, table_name, audit_table)\n\n    audit_rows = []\n\n    try:\n        for year in range(start_year, end_year + 1):\n            for month in range(1, 13):  \n                if month_already_loaded(conn, audit_table, service, year, month):\n                    print(f\"[SKIP] {service} {year}-{month:02d} ya cargado, se omite.\")\n                    continue\n\n                url = _build_url(service, year, month)\n                print(f\"\\n[INFO] Ingestando {service} {year}-{month:02d} desde {url}\")\n\n                row_count = 0\n                try:\n                    resp = requests.get(url, headers=HEADERS, timeout=60)\n                    if resp.status_code == 403:\n                        print(f\"[SKIP] {year}-{month:02d} no disponible (404)\")\n                        audit_rows.append({\n                        \"service\": service,\n                        \"year\": year,\n                        \"month\": month,\n                        \"status\": \"skip\",\n                        \"row_count\": 0,\n                        \"error_message\": \"Archivo no disponible (404)\",\n                        \"run_id\": run_id,\n                        \"ingested_at_utc\": ingested_at,\n                    })\n                        continue\n                    \n                    resp.raise_for_status()\n\n                    parquet_file = pq.ParquetFile(BytesIO(resp.content))\n                    total_rows = parquet_file.metadata.num_rows\n                    total_batches = (total_rows // 1_000_000) + (1 if total_rows % 1_000_000 else 0)\n\n                    print(f\"[INFO] {year}-{month:02d} tiene {total_rows} filas en {total_batches} chunks\")\n\n                    for i, batch in enumerate(parquet_file.iter_batches(batch_size=1_000_000), start=1):\n                        df = batch.to_pandas()\n\n                        # metadata\n                        df[\"RUN_ID\"] = run_id\n                        df[\"VENTANA_TEMPORAL\"] = datetime.now()\n                        df[\"CHUNK_MES\"] = f\"{i}/{month}\"\n                        df[\"YEAR\"] = year\n                        df[\"MONTH\"] = month\n                        df = df.rename(columns=str.upper)\n\n                        for col in [\"TPEP_PICKUP_DATETIME\", \"TPEP_DROPOFF_DATETIME\", \"VENTANA_TEMPORAL\"]:\n                            if col in df.columns: \n                                df[col] = pd.to_datetime(df[col], errors=\"coerce\").dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n\n\n                        # exportar\n                        success, nchunks, nrows, _ = write_pandas(\n                            conn,\n                            df,\n                            table_name=table_name,\n                            auto_create_table=False,\n                            overwrite=False,\n                            quote_identifiers=True,\n                        )\n\n                        row_count += len(df)\n                        restantes = total_batches - i\n                        print(\n                            f\"[EXPORT] {year}-{month:02d} \u2192 chunk {i}/{total_batches} \"\n                            f\"({len(df)} filas, acumulado {row_count}, faltan {restantes})\"\n                        )\n\n                        del df\n                        gc.collect()\n                        time.sleep(0.5)\n\n                    # insertar en audit\n                    cur = conn.cursor()\n                    cur.execute(f\"\"\"\n                        INSERT INTO {audit_table}\n                        (SERVICE, YEAR, MONTH, STATUS, ROW_COUNT, ERROR_MESSAGE, RUN_ID, INGESTED_AT_UTC)\n                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n                    \"\"\", (service, year, month, 'ok', row_count, None, run_id, ingested_at))\n                    cur.close()\n\n                    print(f\"[OK] {year}-{month:02d} exportado \u2192 {row_count} filas totales\")\n\n                except Exception as e:\n                    cur = conn.cursor()\n                    cur.execute(f\"\"\"\n                        INSERT INTO {audit_table}\n                        (SERVICE, YEAR, MONTH, STATUS, ROW_COUNT, ERROR_MESSAGE, RUN_ID, INGESTED_AT_UTC)\n                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n                    \"\"\", (service, year, month, 'fail', 0, str(e), run_id, ingested_at))\n                    cur.close()\n                    print(f\"[ERROR] {year}-{month:02d} \u2192 {e}\")\n\n                time.sleep(1)\n\n    finally:\n        conn.close()\n\n    return {\"audit\": pd.DataFrame(audit_rows)}\n", "file_path": "data_loaders/ingest_yellow_taxis.py", "language": "python", "type": "data_loader", "uuid": "ingest_yellow_taxis"}, "data_loaders/ingest_trips.py:data_loader:python:ingest trips": {"content": "import pandas as pd\nimport pyarrow.parquet as pq\nimport requests\nfrom io import BytesIO\nfrom datetime import datetime\nimport uuid\nimport time\nimport gc\n\nimport snowflake.connector\nfrom snowflake.connector.pandas_tools import write_pandas\n\nfrom mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.data_preparation.decorators import data_loader\nfrom os import path\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n\nBASE_URL = \"https://d37ci6vzurychx.cloudfront.net/trip-data\"\nHEADERS = {\n    \"User-Agent\": \"Mozilla/5.0\",\n    \"Accept\": \"application/octet-stream\",\n}\n\n\ndef _build_url(service: str, year: int, month: int) -> str:\n    return f\"{BASE_URL}/{service}_tripdata_{year}-{month:02d}.parquet\"\n\ndef get_snowflake_conn():\n    config_path = path.join(get_repo_path(), \"io_config.yaml\")\n    config_profile = \"default\"\n    config_loader = ConfigFileLoader(config_path, config_profile)\n\n    return snowflake.connector.connect(\n        user=config_loader.get(\"SNOWFLAKE_USER\"),\n        password=config_loader.get(\"SNOWFLAKE_PASSWORD\"),\n        account=config_loader.get(\"SNOWFLAKE_ACCOUNT\"),\n        warehouse=config_loader.get(\"SNOWFLAKE_DEFAULT_WH\"),\n        database=config_loader.get(\"SNOWFLAKE_DEFAULT_DB\"),\n        schema=config_loader.get(\"SNOWFLAKE_DEFAULT_SCHEMA\"),\n        role=config_loader.get(\"SNOWFLAKE_ROLE\"),\n    )\n\ndef create_table_if_not_exists(conn, table_name: str):\n    create_sql = f\"\"\"\n    CREATE TABLE IF NOT EXISTS {table_name} (\n        VENDORID NUMBER,\n        TPEP_PICKUP_DATETIME TIMESTAMP_NTZ,\n        TPEP_DROPOFF_DATETIME TIMESTAMP_NTZ,\n        PASSENGER_COUNT NUMBER,\n        TRIP_DISTANCE FLOAT,\n        RATECODEID NUMBER,\n        STORE_AND_FWD_FLAG STRING,\n        PULOCATIONID NUMBER,\n        DOLOCATIONID NUMBER,\n        PAYMENT_TYPE NUMBER,\n        FARE_AMOUNT FLOAT,\n        EXTRA FLOAT,\n        MTA_TAX FLOAT,\n        TIP_AMOUNT FLOAT,\n        TOLLS_AMOUNT FLOAT,\n        IMPROVEMENT_SURCHARGE FLOAT,\n        TOTAL_AMOUNT FLOAT,\n        CONGESTION_SURCHARGE FLOAT,\n        AIRPORT_FEE FLOAT,\n        RUN_ID STRING,\n        WINDOW_START TIMESTAMP_NTZ,\n        WINDOW_END TIMESTAMP_NTZ,\n        SERVICE_TYPE STRING,\n        YEAR NUMBER,\n        MONTH NUMBER,\n        SOURCE_URL STRING,\n        INGESTED_AT_UTC TIMESTAMP_NTZ\n    )\n    \"\"\"\n    cur = conn.cursor()\n    cur.execute(create_sql)\n    cur.close()\n\n@data_loader\ndef load_and_export(*args, **kwargs):\n    service = \"yellow\"\n    start_year = int(kwargs.get(\"start_year\", 2015))\n    end_year = int(kwargs.get(\"end_year\", 2015))\n\n    run_id = str(uuid.uuid4())\n    ingested_at = kwargs.get(\"execution_date\") or datetime.utcnow()\n\n    conn = get_snowflake_conn()\n    table_name = \"TRIPS\"\n\n    create_table_if_not_exists(conn, table_name)\n\n    audit_rows = []\n\n    try:\n        for year in range(start_year, end_year + 1):\n            for month in range(1, 2):  # prueba solo enero\n                url = _build_url(service, year, month)\n                print(f\"\\n[INFO] Ingestando {service} {year}-{month:02d} desde {url}\")\n\n                row_count = 0\n                try:\n                    resp = requests.get(url, headers=HEADERS, timeout=60)\n                    resp.raise_for_status()\n\n                    parquet_file = pq.ParquetFile(BytesIO(resp.content))\n                    total_rows = parquet_file.metadata.num_rows\n                    total_batches = (total_rows // 1_000_000) + (1 if total_rows % 1_000_000 else 0)\n\n                    print(f\"[INFO] {year}-{month:02d} tiene {total_rows} filas en {total_batches} chunks\")\n\n                    for i, batch in enumerate(parquet_file.iter_batches(batch_size=1_000_000), start=1):\n                        df = batch.to_pandas()\n\n                        # metadata\n                        df[\"RUN_ID\"] = run_id\n                        df[\"WINDOW_START\"] = datetime(year, month, 1)\n                        df[\"WINDOW_END\"] = (\n                            datetime(year + 1, 1, 1) - pd.Timedelta(seconds=1)\n                            if month == 12\n                            else datetime(year, month + 1, 1) - pd.Timedelta(seconds=1)\n                        )\n                        df[\"SERVICE_TYPE\"] = service\n                        df[\"YEAR\"] = year\n                        df[\"MONTH\"] = month\n                        df[\"SOURCE_URL\"] = url\n                        df[\"INGESTED_AT_UTC\"] = ingested_at\n\n                        # exportar\n                        success, nchunks, nrows, _ = write_pandas(\n                            conn,\n                            df,\n                            table_name=table_name,\n                            auto_create_table=False,\n                            overwrite=False,\n                            quote_identifiers=True,\n                        )\n\n                        row_count += len(df)\n                        restantes = total_batches - i\n                        print(\n                            f\"[EXPORT] {year}-{month:02d} \u2192 chunk {i}/{total_batches} \"\n                            f\"({len(df)} filas, acumulado {row_count}, faltan {restantes})\"\n                        )\n\n                        del df\n                        gc.collect()\n                        time.sleep(0.5)\n\n                    audit_rows.append({\n                        \"service\": service,\n                        \"year\": year,\n                        \"month\": month,\n                        \"status\": \"ok\",\n                        \"row_count\": row_count,\n                        \"error_message\": None,\n                        \"run_id\": run_id,\n                        \"ingested_at_utc\": ingested_at,\n                    })\n                    print(f\"[OK] {year}-{month:02d} exportado \u2192 {row_count} filas totales\")\n\n                except Exception as e:\n                    audit_rows.append({\n                        \"service\": service,\n                        \"year\": year,\n                        \"month\": month,\n                        \"status\": \"fail\",\n                        \"row_count\": 0,\n                        \"error_message\": str(e),\n                        \"run_id\": run_id,\n                        \"ingested_at_utc\": ingested_at,\n                    })\n                    print(f\"[ERROR] {year}-{month:02d} \u2192 {e}\")\n                time.sleep(1)\n\n    finally:\n        conn.close()\n\n    return {\"audit\": pd.DataFrame(audit_rows)}\n", "file_path": "data_loaders/ingest_trips.py", "language": "python", "type": "data_loader", "uuid": "ingest_trips"}, "dbts/dazzling_rogue.yaml:dbt:yaml:dazzling rogue": {"content": "", "file_path": "dbts/dazzling_rogue.yaml", "language": "yaml", "type": "dbt", "uuid": "dazzling_rogue"}, "transformers/revered_field.py:transformer:python:revered field": {"content": "\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nimport pandas as pd\n@transformer\ndef combine_batches(batches, *args, **kwargs):\n    trips_all = []\n    audits_all = []\n\n    for batch in batches:\n        if isinstance(batch, dict):\n            trips_all.append(batch[\"trips\"])\n            audits_all.append(batch[\"audit\"])\n        else:\n            print(f\"[WARN] Batch inv\u00e1lido: {type(batch)}\")\n\n    trips_df = pd.concat(trips_all, ignore_index=True) if trips_all else pd.DataFrame()\n    audit_df = pd.concat(audits_all, ignore_index=True) if audits_all else pd.DataFrame()\n\n    return {\"trips\": trips_df, \"audit\": audit_df}\n", "file_path": "transformers/revered_field.py", "language": "python", "type": "transformer", "uuid": "revered_field"}, "transformers/fill_in_missing_values.py:transformer:python:fill in missing values": {"content": "from pandas import DataFrame\nimport math\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef select_number_columns(df: DataFrame) -> DataFrame:\n    return df[['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Survived']]\n\n\ndef fill_missing_values_with_median(df: DataFrame) -> DataFrame:\n    for col in df.columns:\n        values = sorted(df[col].dropna().tolist())\n        median_value = values[math.floor(len(values) / 2)]\n        df[[col]] = df[[col]].fillna(median_value)\n    return df\n\n\n@transformer\ndef transform_df(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        df (DataFrame): Data frame from parent block.\n\n    Returns:\n        DataFrame: Transformed data frame\n    \"\"\"\n    # Specify your transformation logic here\n\n    return fill_missing_values_with_median(select_number_columns(df))\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "transformers/fill_in_missing_values.py", "language": "python", "type": "transformer", "uuid": "fill_in_missing_values"}, "transformers/transform_data.py:transformer:python:transform data": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\n\nimport pandas as pd\n\n@transformer()\ndef transform_batch(batch, *args, **kwargs):\n    trips = batch[\"trips\"]\n    audit = batch[\"audit\"]\n\n    # hacer validaciones / filtros m\u00ednimos aqu\u00ed\n    return {\"trips\": trips, \"audit\": audit}\n", "file_path": "transformers/transform_data.py", "language": "python", "type": "transformer", "uuid": "transform_data"}, "pipelines/taxi_zones/metadata.yaml:pipeline:yaml:taxi zones/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - export_data\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: ingest_taxi_zones\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: ingest_taxi_zones\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: export_data\n  retry_config: null\n  status: failed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - ingest_taxi_zones\n  uuid: export_data\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-09-25 02:34:35.157195+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: taxi_zones\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: taxi_zones\nvariables_dir: /home/src/mage_data/scheduler\nwidgets: []\n", "file_path": "pipelines/taxi_zones/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "taxi_zones/metadata"}, "pipelines/taxi_zones/__init__.py:pipeline:python:taxi zones/  init  ": {"content": "", "file_path": "pipelines/taxi_zones/__init__.py", "language": "python", "type": "pipeline", "uuid": "taxi_zones/__init__"}, "pipelines/yellow_taxi_trips/metadata.yaml:pipeline:yaml:yellow taxi trips/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_path: data_loaders/ingest_yellow_taxis.py\n    file_source:\n      path: data_loaders/ingest_yellow_taxis.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: ingest_yellow_taxis\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: ingest_yellow_taxis\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-09-18 19:59:14.948040+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: yellow_taxi_trips\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: yellow_taxi_trips\nvariables:\n  end_year: 2025\n  start_year: 2025\nvariables_dir: /home/src/mage_data/scheduler\nwidgets: []\n", "file_path": "pipelines/yellow_taxi_trips/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "yellow_taxi_trips/metadata"}, "pipelines/yellow_taxi_trips/triggers.yaml:pipeline:yaml:yellow taxi trips/triggers": {"content": "triggers: []\n", "file_path": "pipelines/yellow_taxi_trips/triggers.yaml", "language": "yaml", "type": "pipeline", "uuid": "yellow_taxi_trips/triggers"}, "pipelines/yellow_taxi_trips/__init__.py:pipeline:python:yellow taxi trips/  init  ": {"content": "", "file_path": "pipelines/yellow_taxi_trips/__init__.py", "language": "python", "type": "pipeline", "uuid": "yellow_taxi_trips/__init__"}, "pipelines/modeling/metadata.yaml:pipeline:yaml:modeling/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    dbt: {}\n    dbt_profile_target: dev\n    dbt_project_name: dbt/scheduler\n    disable_query_preprocessing: false\n    export_write_policy: append\n    file_path: dbt/scheduler/models/silver/stg_taxi_zones.sql\n    file_source:\n      path: dbt/scheduler/models/silver/stg_taxi_zones.sql\n      project_path: dbt/scheduler\n    limit: 1000\n    use_raw_sql: false\n  downstream_blocks:\n  - dbt/scheduler/models/silver/stg_taxi_trips\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/scheduler/models/silver/stg_taxi_zones\n  retry_config: null\n  status: updated\n  timeout: null\n  type: dbt\n  upstream_blocks: []\n  uuid: dbt/scheduler/models/silver/stg_taxi_zones\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    dbt_project_name: dbt/scheduler\n    file_path: dbt/scheduler/models/silver/stg_taxi_trips.sql\n    file_source:\n      path: dbt/scheduler/models/silver/stg_taxi_trips.sql\n      project_path: dbt/scheduler\n    limit: 1000\n  downstream_blocks:\n  - dbt/scheduler/models/gold/dim_trip_type\n  - dbt/scheduler/models/gold/dim_rate_code\n  - dbt/scheduler/models/gold/dim_vendor\n  - dbt/scheduler/models/gold/dim_service_type\n  - dbt/scheduler/models/gold/dim_payment_type\n  - dbt/scheduler/models/gold/fct_trips\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/scheduler/models/silver/stg_taxi_trips\n  retry_config: null\n  status: updated\n  timeout: null\n  type: dbt\n  upstream_blocks:\n  - dbt/scheduler/models/silver/stg_taxi_zones\n  uuid: dbt/scheduler/models/silver/stg_taxi_trips\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    dbt_project_name: dbt/scheduler\n    file_path: dbt/scheduler/models/gold/dim_date.sql\n    file_source:\n      path: dbt/scheduler/models/gold/dim_date.sql\n      project_path: dbt/scheduler\n    limit: 1000\n  downstream_blocks:\n  - dbt/scheduler/models/gold/fct_trips\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/scheduler/models/gold/dim_date\n  retry_config: null\n  status: updated\n  timeout: null\n  type: dbt\n  upstream_blocks: []\n  uuid: dbt/scheduler/models/gold/dim_date\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    dbt_project_name: dbt/scheduler\n    file_path: dbt/scheduler/models/gold/dim_payment_type.sql\n    file_source:\n      path: dbt/scheduler/models/gold/dim_payment_type.sql\n      project_path: dbt/scheduler\n    limit: 1000\n  downstream_blocks:\n  - dbt/scheduler/models/gold/fct_trips\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/scheduler/models/gold/dim_payment_type\n  retry_config: null\n  status: updated\n  timeout: null\n  type: dbt\n  upstream_blocks:\n  - dbt/scheduler/models/silver/stg_taxi_trips\n  uuid: dbt/scheduler/models/gold/dim_payment_type\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    dbt_project_name: dbt/scheduler\n    file_path: dbt/scheduler/models/gold/dim_rate_code.sql\n    file_source:\n      path: dbt/scheduler/models/gold/dim_rate_code.sql\n      project_path: dbt/scheduler\n  downstream_blocks:\n  - dbt/scheduler/models/gold/fct_trips\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/scheduler/models/gold/dim_rate_code\n  retry_config: null\n  status: updated\n  timeout: null\n  type: dbt\n  upstream_blocks:\n  - dbt/scheduler/models/silver/stg_taxi_trips\n  uuid: dbt/scheduler/models/gold/dim_rate_code\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    dbt_project_name: dbt/scheduler\n    file_path: dbt/scheduler/models/gold/dim_service_type.sql\n    file_source:\n      path: dbt/scheduler/models/gold/dim_service_type.sql\n      project_path: dbt/scheduler\n  downstream_blocks:\n  - dbt/scheduler/models/gold/fct_trips\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/scheduler/models/gold/dim_service_type\n  retry_config: null\n  status: updated\n  timeout: null\n  type: dbt\n  upstream_blocks:\n  - dbt/scheduler/models/silver/stg_taxi_trips\n  uuid: dbt/scheduler/models/gold/dim_service_type\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    dbt_project_name: dbt/scheduler\n    file_path: dbt/scheduler/models/gold/dim_time.sql\n    file_source:\n      path: dbt/scheduler/models/gold/dim_time.sql\n      project_path: dbt/scheduler\n  downstream_blocks:\n  - dbt/scheduler/models/gold/fct_trips\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/scheduler/models/gold/dim_time\n  retry_config: null\n  status: updated\n  timeout: null\n  type: dbt\n  upstream_blocks: []\n  uuid: dbt/scheduler/models/gold/dim_time\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    dbt_project_name: dbt/scheduler\n    file_path: dbt/scheduler/models/gold/dim_trip_type.sql\n    file_source:\n      path: dbt/scheduler/models/gold/dim_trip_type.sql\n      project_path: dbt/scheduler\n  downstream_blocks:\n  - dbt/scheduler/models/gold/fct_trips\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/scheduler/models/gold/dim_trip_type\n  retry_config: null\n  status: updated\n  timeout: null\n  type: dbt\n  upstream_blocks:\n  - dbt/scheduler/models/silver/stg_taxi_trips\n  uuid: dbt/scheduler/models/gold/dim_trip_type\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    dbt_project_name: dbt/scheduler\n    file_path: dbt/scheduler/models/gold/dim_vendor.sql\n    file_source:\n      path: dbt/scheduler/models/gold/dim_vendor.sql\n      project_path: dbt/scheduler\n  downstream_blocks:\n  - dbt/scheduler/models/gold/fct_trips\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/scheduler/models/gold/dim_vendor\n  retry_config: null\n  status: updated\n  timeout: null\n  type: dbt\n  upstream_blocks:\n  - dbt/scheduler/models/silver/stg_taxi_trips\n  uuid: dbt/scheduler/models/gold/dim_vendor\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    dbt_project_name: dbt/scheduler\n    file_path: dbt/scheduler/models/gold/dim_zone.sql\n    file_source:\n      path: dbt/scheduler/models/gold/dim_zone.sql\n      project_path: dbt/scheduler\n  downstream_blocks:\n  - dbt/scheduler/models/gold/fct_trips\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/scheduler/models/gold/dim_zone\n  retry_config: null\n  status: updated\n  timeout: null\n  type: dbt\n  upstream_blocks: []\n  uuid: dbt/scheduler/models/gold/dim_zone\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    dbt_project_name: dbt/scheduler\n    file_path: dbt/scheduler/models/gold/fct_trips.sql\n    file_source:\n      path: dbt/scheduler/models/gold/fct_trips.sql\n      project_path: dbt/scheduler\n    limit: 1000\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/scheduler/models/gold/fct_trips\n  retry_config: null\n  status: updated\n  timeout: null\n  type: dbt\n  upstream_blocks:\n  - dbt/scheduler/models/gold/dim_trip_type\n  - dbt/scheduler/models/gold/dim_rate_code\n  - dbt/scheduler/models/gold/dim_vendor\n  - dbt/scheduler/models/gold/dim_service_type\n  - dbt/scheduler/models/gold/dim_payment_type\n  - dbt/scheduler/models/silver/stg_taxi_trips\n  - dbt/scheduler/models/gold/dim_date\n  - dbt/scheduler/models/gold/dim_time\n  - dbt/scheduler/models/gold/dim_zone\n  uuid: dbt/scheduler/models/gold/fct_trips\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-09-25 03:07:39.435071+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: modeling\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: modeling\nvariables_dir: /home/src/mage_data/scheduler\nwidgets: []\n", "file_path": "pipelines/modeling/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "modeling/metadata"}, "pipelines/modeling/__init__.py:pipeline:python:modeling/  init  ": {"content": "", "file_path": "pipelines/modeling/__init__.py", "language": "python", "type": "pipeline", "uuid": "modeling/__init__"}, "pipelines/green_taxi_trips/metadata.yaml:pipeline:yaml:green taxi trips/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: ingest_taxi\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: ingest_taxi\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-09-23 02:13:51.261434+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: green_taxi_trips\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: green_taxi_trips\nvariables:\n  end_year: 2015\n  start_year: 2015\nvariables_dir: /home/src/mage_data/scheduler\nwidgets: []\n", "file_path": "pipelines/green_taxi_trips/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "green_taxi_trips/metadata"}, "pipelines/green_taxi_trips/__init__.py:pipeline:python:green taxi trips/  init  ": {"content": "", "file_path": "pipelines/green_taxi_trips/__init__.py", "language": "python", "type": "pipeline", "uuid": "green_taxi_trips/__init__"}, "/home/src/scheduler/dbt/scheduler/models/gold/dim_payment_type.sql:dbt:sql:home/src/scheduler/dbt/scheduler/models/gold/dim payment type": {"content": "{{ config(materialized='table') }}\n\nWITH unique_payments AS (\n    SELECT DISTINCT payment_type_desc\n    FROM {{ ref('stg_taxi_trips') }}\n    WHERE payment_type_desc IS NOT NULL\n)\n\nSELECT \n    ROW_NUMBER() OVER (ORDER BY payment_type_desc) AS payment_type_sk,\n    payment_type_desc\nFROM unique_payments", "file_path": "/home/src/scheduler/dbt/scheduler/models/gold/dim_payment_type.sql", "language": "sql", "type": "dbt", "uuid": "dbt/scheduler/models/gold/dim_payment_type"}, "/home/src/scheduler/dbt/scheduler/models/gold/dim_rate_code.sql:dbt:sql:home/src/scheduler/dbt/scheduler/models/gold/dim rate code": {"content": "{{ config(materialized='table') }}\n\nWITH unique_rates AS (\n    SELECT DISTINCT rate_code_id\n    FROM {{ ref('stg_taxi_trips') }}\n    WHERE rate_code_id IS NOT NULL\n)\n\nSELECT \n    ROW_NUMBER() OVER (ORDER BY rate_code_id) AS rate_code_sk,\n    rate_code_id,\n    CASE rate_code_id\n        WHEN 1 THEN 'Standard rate'\n        WHEN 2 THEN 'JFK'\n        WHEN 3 THEN 'Newark'\n        WHEN 4 THEN 'Nassau or Westchester'\n        WHEN 5 THEN 'Negotiated fare'\n        WHEN 6 THEN 'Group ride'\n        ELSE 'Other'\n    END AS rate_code_desc\nFROM unique_rates", "file_path": "/home/src/scheduler/dbt/scheduler/models/gold/dim_rate_code.sql", "language": "sql", "type": "dbt", "uuid": "dbt/scheduler/models/gold/dim_rate_code"}, "/home/src/scheduler/dbt/scheduler/models/gold/dim_service_type.sql:dbt:sql:home/src/scheduler/dbt/scheduler/models/gold/dim service type": {"content": "{{ config(materialized='table') }}\n\nSELECT \n    ROW_NUMBER() OVER (ORDER BY service_type) AS service_type_sk,\n    service_type,\n    CASE service_type\n        WHEN 'yellow' THEN 'Yellow Taxi'\n        WHEN 'green' THEN 'Green Taxi'\n        ELSE 'Other'\n    END AS service_type_desc\nFROM (\n    SELECT DISTINCT service_type\n    FROM {{ ref('stg_taxi_trips') }}\n    WHERE service_type IS NOT NULL\n)", "file_path": "/home/src/scheduler/dbt/scheduler/models/gold/dim_service_type.sql", "language": "sql", "type": "dbt", "uuid": "dbt/scheduler/models/gold/dim_service_type"}, "/home/src/scheduler/dbt/scheduler/models/gold/dim_trip_type.sql:dbt:sql:home/src/scheduler/dbt/scheduler/models/gold/dim trip type": {"content": "{{ config(materialized='table') }}\n\nSELECT\n    ROW_NUMBER() OVER (ORDER BY trip_type) AS trip_type_sk,\n    trip_type,\n    CASE trip_type\n        WHEN 1 THEN 'Street-hail'\n        WHEN 2 THEN 'Dispatch'\n        ELSE 'Unknown'\n    END AS trip_type_desc\nFROM (\n    SELECT DISTINCT trip_type\n    FROM {{ ref('stg_taxi_trips') }}\n    WHERE trip_type IS NOT NULL\n)", "file_path": "/home/src/scheduler/dbt/scheduler/models/gold/dim_trip_type.sql", "language": "sql", "type": "dbt", "uuid": "dbt/scheduler/models/gold/dim_trip_type"}, "/home/src/scheduler/dbt/scheduler/models/gold/dim_date.sql:dbt:sql:home/src/scheduler/dbt/scheduler/models/gold/dim date": {"content": "{{ config(materialized='table') }}\n\nSELECT\n    ROW_NUMBER() OVER (ORDER BY full_date) AS date_sk,\n    full_date,\n    EXTRACT(YEAR FROM full_date) AS year,\n    EXTRACT(MONTH FROM full_date) AS month,\n    EXTRACT(DAY FROM full_date) AS day,\n    EXTRACT(DOW FROM full_date) AS day_of_week,\n    CASE EXTRACT(DOW FROM full_date)\n        WHEN 0 THEN 'Sunday'\n        WHEN 1 THEN 'Monday'\n        WHEN 2 THEN 'Tuesday'\n        WHEN 3 THEN 'Wednesday'\n        WHEN 4 THEN 'Thursday'\n        WHEN 5 THEN 'Friday'\n        WHEN 6 THEN 'Saturday'\n    END AS day_name\nFROM (\n    SELECT DISTINCT DATE(pickup_datetime) AS full_date\n    FROM {{ ref('stg_taxi_trips') }}\n    UNION\n    SELECT DISTINCT DATE(dropoff_datetime)\n    FROM {{ ref('stg_taxi_trips') }}\n)", "file_path": "/home/src/scheduler/dbt/scheduler/models/gold/dim_date.sql", "language": "sql", "type": "dbt", "uuid": "dbt/scheduler/models/gold/dim_date"}, "/home/src/scheduler/dbt/scheduler/models/gold/fct_trips.sql:dbt:sql:home/src/scheduler/dbt/scheduler/models/gold/fct trips": {"content": "{{ config(\n    materialized='incremental',\n    unique_key='trip_id',\n    on_schema_change='sync_all_columns'\n) }}\n\nWITH base AS (\n    SELECT\n        MD5(CONCAT(\n            CAST(pickup_datetime AS VARCHAR),\n            CAST(pu_location_id AS VARCHAR),\n            CAST(do_location_id AS VARCHAR),\n            CAST(vendor_id AS VARCHAR)\n        )) AS trip_id,\n        \n        DATE(pickup_datetime) AS pickup_date,\n        pickup_datetime,\n        dropoff_datetime,\n        \n        vendor_id,\n        rate_code_id,\n        payment_type_desc,\n        service_type,\n        trip_type,\n        pu_location_id,\n        do_location_id,\n        \n        passenger_count,\n        trip_distance,\n        fare_amount,\n        tip_amount,\n        tolls_amount,\n        extra,\n        mta_tax,\n        improvement_surcharge,\n        congestion_surcharge,\n        COALESCE(airport_fee, 0) AS airport_fee,\n        COALESCE(cbd_congestion_fee, 0) AS cbd_congestion_fee,\n        \n        DATEDIFF('minute', pickup_datetime, dropoff_datetime) AS trip_duration_min,\n        CASE \n            WHEN DATEDIFF('minute', pickup_datetime, dropoff_datetime) > 0\n            THEN (trip_distance / (DATEDIFF('minute', pickup_datetime, dropoff_datetime) / 60.0))\n            ELSE 0\n        END AS mph\n        \n    FROM {{ ref('stg_taxi_trips') }}\n    \n    WHERE 1=1\n      -- Filtros de calidad\n      AND DATEDIFF('minute', pickup_datetime, dropoff_datetime) > 0\n      AND DATEDIFF('minute', pickup_datetime, dropoff_datetime) < 1440  \n      AND trip_distance > 0\n      AND trip_distance < 100  \n      AND fare_amount >= 0\n      AND fare_amount < 1000  \n    \n    {% if is_incremental() %}\n      AND pickup_datetime > (SELECT COALESCE(MAX(pickup_datetime), '2000-01-01'::TIMESTAMP) FROM {{ this }})\n    {% endif %}\n    \n    {% if var('process_month', none) %}\n      AND DATE_TRUNC('month', pickup_datetime) = '{{ var(\"process_month\") }}'::DATE\n    {% endif %}\n)\n\nSELECT\n\n    b.trip_id,\n    \n    d.date_sk AS pickup_date_sk,\n    d.date_sk AS dropoff_date_sk,  -\n    EXTRACT(HOUR FROM b.pickup_datetime) AS pickup_time_sk,  \n    EXTRACT(HOUR FROM b.dropoff_datetime) AS dropoff_time_sk, \n    b.pu_location_id AS pu_zone_sk,\n    b.do_location_id AS do_zone_sk,\n    v.vendor_sk,\n    r.rate_code_sk,\n    p.payment_type_sk,\n    s.service_type_sk,\n    COALESCE(tt.trip_type_sk, -1) AS trip_type_sk,\n    \n    b.pickup_datetime,\n    b.dropoff_datetime,\n    \n    EXTRACT(YEAR FROM b.pickup_date) AS pickup_year,\n    EXTRACT(MONTH FROM b.pickup_date) AS pickup_month,\n    EXTRACT(DAY FROM b.pickup_date) AS pickup_day,\n    EXTRACT(DOW FROM b.pickup_date) AS pickup_dow,\n    EXTRACT(HOUR FROM b.pickup_datetime) AS pickup_hour,\n    \n    b.passenger_count,\n    b.trip_distance,\n    b.fare_amount,\n    b.tip_amount,\n    b.tolls_amount,\n    b.extra,\n    b.mta_tax,\n    b.improvement_surcharge,\n    b.congestion_surcharge,\n    b.airport_fee,\n    b.cbd_congestion_fee,\n    \n    (b.fare_amount + b.tip_amount + b.tolls_amount + b.extra + b.mta_tax +\n     b.improvement_surcharge + b.congestion_surcharge + b.airport_fee + b.cbd_congestion_fee\n    ) AS total_amount,\n    \n    CASE \n        WHEN b.fare_amount > 0 \n        THEN (b.tip_amount / b.fare_amount) * 100\n        ELSE 0\n    END AS tip_percentage,\n    \n    b.trip_duration_min,\n    b.mph\n\nFROM base b\nLEFT JOIN {{ ref('dim_date') }} d ON b.pickup_date = d.full_date\nLEFT JOIN {{ ref('dim_vendor') }} v ON b.vendor_id = v.vendor_id\nLEFT JOIN {{ ref('dim_rate_code') }} r ON b.rate_code_id = r.rate_code_id\nLEFT JOIN {{ ref('dim_payment_type') }} p ON b.payment_type_desc = p.payment_type_desc\nLEFT JOIN {{ ref('dim_service_type') }} s ON b.service_type = s.service_type\nLEFT JOIN {{ ref('dim_trip_type') }} tt ON b.trip_type = tt.trip_type", "file_path": "/home/src/scheduler/dbt/scheduler/models/gold/fct_trips.sql", "language": "sql", "type": "dbt", "uuid": "dbt/scheduler/models/gold/fct_trips"}, "/home/src/scheduler/dbt/scheduler/models/gold/dim_time.sql:dbt:sql:home/src/scheduler/dbt/scheduler/models/gold/dim time": {"content": "{{ config(materialized='table') }}\n\nWITH times AS (\n    SELECT DISTINCT CAST(pickup_datetime AS TIME) AS time_of_day\n    FROM {{ ref('stg_taxi_trips') }}\n    UNION\n    SELECT DISTINCT CAST(dropoff_datetime AS TIME)\n    FROM {{ ref('stg_taxi_trips') }}\n)\n\nSELECT\n    ROW_NUMBER() OVER (ORDER BY time_of_day) AS time_sk,\n    time_of_day,\n    EXTRACT(HOUR FROM time_of_day) AS hour,\n    CASE \n        WHEN EXTRACT(HOUR FROM time_of_day) BETWEEN 6 AND 11 THEN 'Morning'\n        WHEN EXTRACT(HOUR FROM time_of_day) BETWEEN 12 AND 17 THEN 'Afternoon'\n        WHEN EXTRACT(HOUR FROM time_of_day) BETWEEN 18 AND 21 THEN 'Evening'\n        ELSE 'Night'\n    END AS time_period,\n    CASE \n        WHEN EXTRACT(HOUR FROM time_of_day) BETWEEN 6 AND 18 THEN 'Day'\n        ELSE 'Night' \n    END AS day_night\nFROM times", "file_path": "/home/src/scheduler/dbt/scheduler/models/gold/dim_time.sql", "language": "sql", "type": "dbt", "uuid": "dbt/scheduler/models/gold/dim_time"}}, "custom_block_template": {}, "mage_template": {"data_loaders/airtable.py:data_loader:python:Airtable:Load a Table from Airtable App.": {"block_type": "data_loader", "description": "Load a Table from Airtable App.", "language": "python", "name": "Airtable", "path": "data_loaders/airtable.py"}, "data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}